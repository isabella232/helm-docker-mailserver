---
image:
  # image.name is the name of the container image to use. Refer to https://hub.docker.com/r/mailserver/docker-mailserver
  name: "mailserver/docker-mailserver"
  # image.tag is the tag of the container image to use. Refer to https://hub.docker.com/r/mailserver/docker-mailserver
  tag: "11.0.0"
  pullPolicy: "IfNotPresent"

initContainer:
  image:
    name: "busybox"
    tag: "stable"
    pullPolicy: "IfNotPresent"

  # These resources refer specifically to the _init container_, which needs only _puny_ resources,
  # since all it does is copy the config into place
  resources:
    requests:
      cpu: "10m"
      memory: "32Mi"
    limits:
      cpu: "50m"
      memory: "64Mi"

  containerSecurityContext:
    readOnlyRootFilesystem: true
    privileged: false

## Optionally specify a runtimeClassName for the deployment
runtimeClassName: 

## Optionally specify a priorityClassName for the deployment
priorityClassName:

serviceAccount:
  create: "true"

## By default, the chart will create a configmap based on the contents of the config/ subdirectory
## If you want to create the configmap yourself, set useExisting to "true", and specify the name
## of the existing configmap.
## This is an advanced feature, allowing further customization / templating of config elements
##
configMap:
  useExisting: false

secret:
  useExisting: false  

## These values define how the certificate created by the chart is processed by cert-manager
## Ensure you've setup your ClusterIssuers (letsencrypt-staging and letsencrypt-prod) and DNS provider
ssl:
  issuer:
    name: letsencrypt-staging
    kind: ClusterIssuer
  dnsname: example.com
  dns01provider: cloudflare

  # Set this to false to let the chart try to assemble a CR for you. This feature will be deprecated soon, and users will 
  # need to manage cert-manager outside of this chart (the dependencies involved make it fragile)
  useExisting: true 
  existingName: name-of-existing-secret

## Mount additional volumes into the container.  Useful for persistent logs or
## injecting additional config files.
#additionalVolumes:
#  - name: "additional"
#    emptyDir: {}
#additionalVolumeMounts:
#  - name: additional
#    mountPath: /additional

demoMode:
  # demoMode.enabled ignores the contents of helm-chart/docker-mailserver/config and creates minimal static configuration to be used instead. A single account (user "user@example.com", password "password") will be configured for the purpose of validating core functions and connectivity, before applying user config
  enabled: true

# If you choose _not_ to use haproxy, and you're not exposing your services with a load-balanced service
# with an external traffic policy in "Local" mode, you risk having the source IP of incoming mail overwritten
# with a local Kubernetes cluster IP, as part of the ingress routing of the connection. This, in turn, 
# will cause all incoming to appear to be coming from an internal IP, causing SPF tests to fail.
# Disable the following to bypass SPF tests altogether, to accommodate this scenario.
# spfTestsDisabled will ignore all SPF-based spam tests, in the event that you cannot obtain valid source IP details on ingress emails
spfTestsDisabled: false

# List extra RBL domains to use for hard reject filtering
rblRejectDomains: []

# List all the domains to be used below - this is necessary to correctly configure DKIM keys for email signing
# domains is the list of domains to be served by your docker-mailserver instance
domains: []
#  - your-first-domain.com
#  - your-second-domain.com

livenessTests:
  # livenessTests.enabled will add a liveness test, which will classify a pod as 'unhealthy' if any livenessTests.commands (below) return non-zero
  enabled: true
  # livenessTests.commands is an array of commands to be executed within the docker-mailserver container, intended to prove that the container is healthy. Each command must exit 0 under normal (healthy) circumstances
  commands:
    - "clamscan /tmp/docker-mailserver/TrustedHosts"

pod:
  # pod.dockermailserver section refers to the configuration of the docker-mailserver pod itself. 

  # Note that the many environment variables which define the behaviour of docker-mailserver are configured here
  # See https://github.com/docker-mailserver/docker-mailserver/blob/master/ENVIRONMENT.md for details
  dockermailserver:

    ## Host networking requested for this pod. Use the host’s network namespace. If this option is set, the ports that
    ## will be used must be specified.
    ## Ref: https://kubernetes.io/docs/api-reference/v1/definitions/#_v1_podspec
    # pod.dockermailserver.hostNetwork will configure the pod to use the host's network namespace
    hostNetwork: false
    ## Use the host’s pid namespace
    ## Ref: https://kubernetes.io/docs/api-reference/v1/definitions/#_v1_podspec
    # pod.dockermailserver.hostPID defines whether the pod should use the host's PID namespace (default false)
    hostPID: false

    ## Update strategy - only really applicable for deployments with RWO PVs attached
    ## If replicas = 1, an update can get "stuck", as the previous pod remains attached to the 
    ## PV, and the "incoming" pod can never start. Setting the strategy to "Recreate" (our default) will 
    ## terminate the single previous pod, so that the new, incoming pod can attach to the PV    
    strategy:
      # rollingUpdate:
      #   maxSurge: 1
      #   maxUnavailable: 1
      type: "Recreate"

    ## The following variables affect the behaviour of docker-mailserver
    ## See https://docker-mailserver.github.io/docker-mailserver/v11.0/config/environment/ for details
    ## Note that an empty value indicates the default as described in the docs above
    env:
      # General
      OVERRIDE_HOSTNAME: "mail.batcave.org"
      LOG_LEVEL: info
      ONE_DIR: 1
      PERMIT_DOCKER:
      TZ: 
      ENABLE_AMAVIS: 1
      AMAVIS_LOGLEVEL: 0
      ENABLE_DNSBL: 0
      ENABLE_CLAMAV: 0
      ENABLE_POP3:
      ENABLE_FAIL2BAN: 0 # Setting this to 1 will add the NET_ADMIN cap to the deployment
      FAIL2BAN_BLOCKTYPE: drop
      SMTP_ONLY:
      SSL_TYPE: manual
      SSL_CERT_PATH:
      SSL_KEY_PATH:
      TLS_LEVEL: 
      SPOOF_PROTECTION:
      ENABLE_SRS: 0
      NETWORK_INTERFACE: 
      VIRUSMAILS_DELETE_DELAY:
      ENABLE_POSTFIX_VIRTUAL_TRANSPORT:
      POSTFIX_DAGENT: 
      POSTFIX_MAILBOX_SIZE_LIMIT:
      ENABLE_QUOTAS: 1
      POSTFIX_MESSAGE_SIZE_LIMIT:
      CLAMAV_MESSAGE_SIZE_LIMIT:
      ENABLE_MANAGESIEVE:
      POSTMASTER_ADDRESS:
      ENABLE_UPDATE_CHECK: 1
      UPDATE_CHECK_INTERVAL: 1d
      POSTSCREEN_ACTION: enforce
      DOVECOT_MAILBOX_FORMAT: maildir
      POSTFIX_INET_PROTOCOLS: all
      DOVECOT_INET_PROTOCOLS: all
      # Reports
      PFLOGSUMM_TRIGGER:
      PFLOGSUMM_RECIPIENT: 
      PFLOGSUMM_SENDER:
      LOGWATCH_INTERVAL: 
      LOGWATCH_RECIPIENT:
      LOGWATCH_SENDER:
      REPORT_RECIPIENT:
      REPORT_SENDER:
      LOGROTATE_INTERVAL: weekly
      # SpamAssassin
      ENABLE_SPAMASSASSIN: 0
      SPAMASSASSIN_SPAM_TO_INBOX: 1
      ENABLE_SPAMASSASSIN_KAM: 0
      MOVE_SPAM_TO_JUNK: 1
      SA_TAG: 2.0
      SA_TAG2: 6.31
      SA_KILL: 6.31
      SA_SPAM_SUBJECT: "**SPAM**"
      SA_SHORTCIRCUIT_BAYES_SPAM: 1
      SA_SHORTCIRCUIT_BAYES_HAM: 1
      # Fetchmail
      ENABLE_FETCHMAIL: 0
      FETCHMAIL_POLL: 300
      FETCHMAIL_PARALLEL: 0
      # LDAP
      ENABLE_LDAP: 
      LDAP_START_TLS: 
      LDAP_SERVER_HOST: 
      LDAP_SEARCH_BASE:
      LDAP_BIND_DN:
      LDAP_BIND_PW:
      LDAP_QUERY_FILTER_USER:
      LDAP_QUERY_FILTER_GROUP:
      LDAP_QUERY_FILTER_ALIAS:
      LDAP_QUERY_FILTER_DOMAIN:
      LDAP_QUERY_FILTER_SENDERS:
      DOVECOT_TLS:
      # Dovecot
      DOVECOT_BASE:
      DOVECOT_DEFAULT_PASS_SCHEME:
      DOVECOT_DN:
      DOVECOT_DNPASS:
      DOVECOT_URIS:
      DOVECOT_LDAP_VERSION:
      DOVECOT_AUTH_BIND:
      DOVECOT_USER_FILTER:
      DOVECOT_USER_ATTRS:
      DOVECOT_PASS_FILTER:
      DOVECOT_PASS_ATTRS:
      # Postgrey
      ENABLE_POSTGREY:
      POSTGREY_DELAY: 300
      POSTGREY_MAX_AGE: 35
      POSTGREY_AUTO_WHITELIST_CLIENTS: 5
      POSTGREY_TEXT: Delayed by Postgrey
      # SASL Auth
      ENABLE_SASLAUTHD:
      SASLAUTHD_MECHANISMS:
      SASLAUTHD_MECH_OPTIONS:
      SASLAUTHD_LDAP_SERVER:
      SASLAUTHD_LDAP_START_TLS:
      SASLAUTHD_LDAP_TLS_CHECK_PEER:
      SASLAUTHD_LDAP_TLS_CACERT_DIR:
      SASLAUTHD_LDAP_TLS_CACERT_FILE:
      SASLAUTHD_LDAP_BIND_DN:
      SASLAUTHD_LDAP_PASSWORD:
      SASLAUTHD_LDAP_SEARCH_BASE:
      SASLAUTHD_LDAP_FILTER:
      SASLAUTHD_LDAP_PASSWORD_ATTR:
      SASL_PASSWD:
      SASLAUTHD_LDAP_AUTH_METHOD:
      SASLAUTHD_LDAP_MECH:
      # SRS (Sender Rewriting Scheme)
      SRS_SENDER_CLASSES: envelope_sender
      SRS_EXCLUDE_DOMAINS:
      SRS_SECRET:
      SRS_DOMAINNAME:
      # Default Relay Host
      DEFAULT_RELAY_HOST:
      # Multi-domain Relay Hosts
      RELAY_HOST:
      RELAY_PORT:
      RELAY_USER:
      RELAY_PASSWORD:

    # Whether to enable dovecot replication. Allows the syncronization of a pair of dovecot servers
    # https://wiki.dovecot.org/Replication
    enable_dovecot_replication: true

    securityContext:
      runAsUser: 10001
      runAsGroup: 10001

    containerSecurityContext:
      readOnlyRootFilesystem: false # incompatible with the way docker-mailserver works
      privileged: false

service:
  ## What scope the service should be exposed in. One of:
  ## - LoadBalancer (to the world)
  ## - ClusterIP (to the cluster)
  type: "LoadBalancer"
  ## If there is a port associated with a given service, expose it here.
  # port:
  ## If there is a particular IP that should be used for the service, specify it here.
  ## Note: It's quite unlikely that an IP should be specific. Normally, the best thing to do is leave it to Kubernetes
  ##       to allocate a free IP from the pool.
  ## Default: Automatically assign a random IP
  # privateIp:
  ## Only relevant if the `type` above is "LoadBalancer"
  loadBalancer:
    ## If there is already a reserved public IP that this load balancer should use, indicate it here.
    ## Default: Automatically assign a random, ephemeral IP
    # publicIp:
    ## If there should be firewall rules restricting the load balancer to a limited set of IPs, specify those IPs below
    ## in CIDR format. If all IPs shoud be allowed access, set the CIDR as "0.0.0.0/0"
    allowedIps:
      - "0.0.0.0/0"
    ## If there is a Hostname associated with this site, add it here and it will be rendered in the documentation.
    # hostName:
  annotations: {}

deployment:

  ## How many versions of the deployment to run on kubernetes
  ## Default: 2
  replicas: 1

  ## Add annotations to the deployment
  ## Useful for using something like stash to backup data (https://stash.run/docs/v0.9.0-rc.0/guides/latest/auto-backup/workload/)
  annotations: {}  

## More generally, a "request" can be thought of as "how much is this container expected to need usually". it should be
## possible to burst outside these constraints (during a high load operation). However, Kubernetes may kill the pod
## if the node is under too higher load and the burst is outside its request
##
## Limits are hard limits. Violating them is either impossible, or results in container death. I'm not sure whether
## making these optional is a good idea or not; at the moment, I think I'm happy to defer QOS to the cluster and try
## and keep requests close to usage.
##
## Requests are what are used to determine whether more software "fits" onto the cluster.
##
## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
## Ref: https://github.com/kubernetes/kubernetes/blob/master/docs/design/resource-qos.md
## Ref: https://docs.docker.com/engine/reference/run/#/runtime-constraints-on-resources
resources:
  requests:
    ## How much CPU this container is expected to need
    cpu: "1"
    ## How much memory this container is expected to need.
    ## Reduce these at requests your peril - too few resources can cause daemons (i.e., clamd) to fail, or timeouts to occur.
    ## A test installation with clamd running was killed when it consumed 1437Mi (which is why this value was increased to 1536)
    memory: "1536Mi"
  limits:
    ## The max CPU this container should be allowed to use
    cpu: "2"
    ## The max memory this container should be allowed to use. Note: If a container exceeds its memory limit,
    ## it may terminated.
    memory: "2048Mi"

persistence:
  size: "10Gi"
  # Uncomment the backup.kubernetes.io/deltas annotation below if you use https://github.com/miracle2k/k8s-snapshots
  annotations: {}
    # backup.kubernetes.io/deltas: PT1H P2D P30D P180D

## Monitoring adds the prometheus.io annotations to pods and services, so that the Prometheus Kubernetes SD mechanism
## as configured in the examples will automatically discover both the pods and the services to query.
##
## This defaults on, as the annotations should do no harm where Prometheus is not available but will automatically
## expose the application where Prometheus is.
##
## See https://github.com/prometheus/docs/blob/master/content/docs/operating/configuration.md
## See https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml
monitoring:
  ## Whether to scrape this service with the montoring toolkit. Mostly useful for blackbox probing of a given service
  ## to ensure it's "up"
  service:
    ## monitoring should be configured to only scrape services that have a value of "true"
    scrape: "true"
    ## monitoring should be configured to only probe services that have a value of "true"
    probe: "false"    
    ## Path on which metrics are exposed
    path: "/metrics"
    ## Port on which HTTP server is served
    port: "9102"
  ## Whether to scape the pods associated with this application. Useful for collecting metrics.
  pod:
    ## monitoring shoudl be configured to only scrape pods that have a value of `true`
    scrape: "true"
    ## monitoring should be configured to only probe services that have a value of "true"
    probe: "false"    
    ## Path on which metrics are exposed
    path: "/metrics"
    ## Port on which HTTP server is served
    port: "9102"

# Values imported from https://github.com/t13a/helm-chart-rainloop/blob/master/values.yaml
rainloop:
  # rainloop.enabled will include a rainloop (webmail) pod in the release
  enabled: false
  serviceAccount:
    create: true
    name: "rainloop"
  image:
    # rainloop.image.name is the docker container to use for the rainloop pod
    name: "hardware/rainloop"
    # rainloop.image.tag is the tag of the docker container to use for the rainloop pod
    tag: "latest"
    pullPolicy: "Always"

    ## Update strategy - only really applicable for deployments with RWO PVs attached
    ## If replicas = 1, an update can get "stuck", as the previous pod remains attached to the 
    ## PV, and the "incoming" pod can never start. Setting the strategy to "Recreate" (our default) will 
    ## terminate the single previous pod, so that the new, incoming pod can attach to the PV.    
    strategy:
      # rollingUpdate:
      #   maxSurge: 1
      #   maxUnavailable: 1
      type: "Recreate"
          
  persistence:
    size: "1Gi"
    # Uncomment the backup.kubernetes.io/deltas annotation below if you use https://github.com/miracle2k/k8s-snapshots
    annotations: {}
      # backup.kubernetes.io/deltas: PT1H P2D P30D P180D    
  service:
    port: 80
  container:
    port: 8888
  ingress:
    enabled: true
    hosts:
      - rainloop.example.com
    annotations: {}
  # kubernetes.io/ingress.class: nginx
  # kubernetes.io/tls-acme: "true"
    path: /
    tls: []

## These values are for the haproxy sub-chart
haproxy:
  # haproxy.enabled will deploy an haproxy sub-chart, configured for the TCP ports used by docker-mailserver
  enabled: false
  controller:
    replicaCount: 1
    kind: "Deployment"
    enableStaticPorts: false
    tcp:
      25: "default/docker-mailserver:25::PROXY-V1"
      110: "default/docker-mailserver:110::PROXY-V1"
      143: "default/docker-mailserver:143::PROXY-V1"            
      465: "default/docker-mailserver:465"
      587: "default/docker-mailserver:587"
      993: "default/docker-mailserver:993::PROXY-V1"  
      995: "default/docker-mailserver:995::PROXY-V1"     
    service:
      externalTrafficPolicy: "Local"
    # Set to avoid CI error when running the generated manifest through kubeval (FIXME)
    podAnnotations: 
      set-to-avoid-lint-errors-in: "docker-mailserver"

  defaultBackend:
    replicaCount: 1

  # These values populate dovecot's list of networks it'll "trust" for incoming haproxy connections.
  # A space-separated list, on a single line, is required
  # By default, we allow all RFC1918 private ranges, but this can be tightened up for the known IP/range
  # of your HAProxy instance
  # haproxy.trustedNetworks is the list of sources (in CIDR format, space-separated) to permit haproxy PROXY protocol from
  trustedNetworks: "10.0.0.0/8 192.168.0.0/16 172.16.0.0/16"